# 목차

1. 선형회귀: 입력과 출력의 선형 관계를 찾는 방법
2. 단순선형회귀: 하나의 입력 변수로 출력 예측하기
3. 다중선형회귀: 여러 입력 변수를 활용한 예측
4. 선형회귀 주의사항: 변수 상관(다중공선성), 상관과 인과 구분

# 학습 목표

- 선형회귀의 핵심 개념 이해: 입력-출력의 선형 관계, 단순선형회귀 이해
- 모델 적합: 잔차제곱합(RSS)과 최소 제공 이해
- 다중선형회귀로 확장한 학습 방법 이해

# 1. 선형회귀

## 1-1. 선형회귀(Linear Regression)

- 입력 변수와 출력 변수 사이의 관계를 직선 형태로 표현한 것
- 과거의 관계를 기반으로 새로운 데이터를 예측하기 위해 사용하는 모델링 방법 중 하나
- 지도학습의 가장 기초가 되는 접근 중 하나

# 2. 단순선형회귀(Simple Linear Regression)

- 한 개의 설명변수(x)와 하나의 반응변수(y) 사이의 선형(직선) 관계를 찾는 방법

예시

- 설명변수 x: 전체 음식 값
- 반응변수 y: 팁 금액

예시 이미지
<img src="./img/1.png">

➡️ 해당 그래프의 핑크색 선은 '전체 음식 값' 과 '팁 금액' 간의 관계를 가장 잘 표현할 수 있는 모델, 즉 식이다.

단일 설명변수를 이용한 단순선형회귀는 다음 식을 나타낼 수 있다.

## 2-1. 모델

$$
Y = \beta_0(y 절편) + \beta_1(기울기)X + ε(관측 오차)
$$

- `y절편`: x = 0 일 때 y 값. 예제에서는 식당에서 지불한 비용이 0원일 때 내는 팁의 값
- `기울기`: x 가 1단위 증가할 때 y 의 평균 증가량

## 2-2. 최소 제곱법(least squares)

각 데이터들의 관계를 표현하는 모델이라면, 오차를 가장 적게 내는 직선을 찾는 것이 관건이다. 그 방법으로는 최소제곱법이 있다.

- 실제 관측값과 예측값의 차이를 제곱해 합한 값(RSS, 잔차제곱합)을 최소화하는 법

**잔차(residual) 정의**

$$
e_i = y_i - y\hat{}_i(예측 값)
$$

참고) 예측 값 식으로 보기

$$
(y\hat{}_i = \beta\hat{}_0 + \beta\hat{}_1x_i)
$$

**RSS(잔차제곱합) 정의**

$$
RSS = e_1^2 + e_2^2 + ... + e_n^2
$$

**다른 표현**

$$
RSS = \displaystyle\sum_{i=1}^{n}{ e_i^2} = \displaystyle\sum_{i=1}^{n}{(y_i - y\hat{}_i)^2} = \displaystyle\sum_{i=1}^{n}{(y_i - \beta\hat{}_0 + \beta\hat{}_1x_i)^2}
$$

**계수를 측정하기 위한 공식**: closed-form solution

$$
\beta_1\hat{} = \frac{\displaystyle\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\displaystyle\sum_{i=1}^{n}{(x_i - \bar{x}})^2}
$$

**절편**

$$
\beta_0\hat{} = \bar{y} - \beta_1\hat{}\bar{x}
$$

- RSS 를 최소화하는 절편 $\beta_0\hat{}$ 을 찾기 위해 미분(기울기)이 0이 되는 지점을 구함
- 회귀식 자체는 1차식이고, 잔차도 1차식이다. 따라서 잔차를 제곱한 함수는 2차함수가 되고, 일반적으로 아래로 볼록한 포물선 형태를 가진다. 따라서 기울기가 0인 지점이 함수의 최솟값이 되고, 곧 오차의 최솟값을 가진다.

- $\bar{x}$: x들의 평균값
- $\bar{y}$: y들의 평균값

> 나중에 경사하강법에서 선형회귀 푸는 방법을 다룰 예정
